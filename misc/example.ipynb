{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33eca37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marawangamal/Documents/github/ctrl-rlvr/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import ctrlg\n",
    "\n",
    "# Defin model\n",
    "model_name = \"ctrlg/gpt2-large_common-gen\"\n",
    "hmm_model_name = f'ctrlg/hmm_gpt2-large_common-gen_4096' # alternatively ctrlg/hmm_gpt2-large_common-gen_4096 for better quality\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define HMM\n",
    "hmm_model = ctrlg.HMM.from_pretrained(hmm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c91b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Optional\n",
    "\n",
    "def get_dfa_model_v2(\n",
    "        hmm_model: torch.nn.Module,\n",
    "        prompt_ids: List[int],   # Shape: (B, T)\n",
    "        tokenizer:AutoTokenizer,\n",
    "        keyphrases:List[List[str]]=[[' ']], \n",
    "        suffix_ids:Optional[List[int]]=None, \n",
    "        min_new_tokens:int=5, \n",
    "        max_new_tokens:int=32,\n",
    "        device:torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ):\n",
    "    \"\"\"Constructs a DFA model for the given prompt and keyphrases.\n",
    "\n",
    "    Args:\n",
    "        prompt_ids (List[int]): Prompt integer list\n",
    "        keyphrases (List[List[str]], optional): List of keyphrases to be constrained. Defaults to [[' ']].\n",
    "        suffix_ids (Optional[List[int]], optional): Suffix integer list. Defaults to None.\n",
    "        min_new_tokens (int, optional): Minimum number of tokens to generate. Defaults to 5.\n",
    "        max_new_tokens (int, optional): Maximum number of tokens to generate. Defaults to 32.\n",
    "        device (torch.device, optional): Device to run the model on. Defaults to torch.device('cuda' if torch.cuda.is_available() else 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        constraint_logits_processor: Logits processor for the DFA model.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_size = len(tokenizer)\n",
    "\n",
    "    ##################################### prefix, suffix, prompt #####################################\n",
    "    prefix = '' # generate text starting with nothing\n",
    "    suffix = '.<|endoftext|>' # generate text ending with '<|endoftext|>'; a suffix must end with the eos token\n",
    "\n",
    "    prefix_ids = tokenizer.encode(prefix)\n",
    "    if suffix_ids is None:\n",
    "        suffix_ids = tokenizer.encode(suffix)\n",
    "\n",
    "    ##################################### DFA Construction #####################################\n",
    "    # ac_builder constructs a DFA representing the constraint that (at least) \n",
    "    # one the patterns must appear; a pattern is a sequence of token ids\n",
    "    ac_builder = ctrlg.AhoCorasickBuilder(vocab_size)\n",
    "\n",
    "    dfa_graphs = []\n",
    "\n",
    "    # constraint 1:\n",
    "    for keyphrase in keyphrases:\n",
    "        patterns = [tokenizer.encode(x) for x in keyphrase]\n",
    "        dfa_graphs.append(ac_builder.build(patterns))\n",
    "\n",
    "    # taking the intersection of the DFAs, i.e., \"logical and\" of the constraints.\n",
    "    # This function also minimizes the constructed DFA, which is mainly CPU-based operations;\n",
    "    # Due to its pure python implemenation, DFA minimization can be slow for complex constraints\n",
    "    dfa_graph = ctrlg.DFA_prod(dfa_graphs, mode='intersection')\n",
    "\n",
    "    # compile the dfa_graph for efficient GPU execution\n",
    "    dfa_model = ctrlg.DFAModel(dfa_graph, vocab_size).to(device)\n",
    "\n",
    "    ##################################### token length #####################################\n",
    "\n",
    "    constraint_logits_processor = ctrlg.ConstraintLogitsProcessor(\n",
    "        hmm_model, \n",
    "        dfa_model,\n",
    "        min_new_tokens, \n",
    "        max_new_tokens,\n",
    "        prompt_ids, \n",
    "        prefix_ids=prefix_ids, \n",
    "        suffix_ids=suffix_ids\n",
    "    )\n",
    "\n",
    "    return constraint_logits_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cfa60cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Places\n",
    "prompt = 'What is 10+2?'\n",
    "solution = \" 13 \"\n",
    "\n",
    "# Math\n",
    "# prompt = \"To express 20 as a sum of different powers of 2, we would write $20 = 2^4 + 2^2$. The sum of the exponents of these powers is $4 + 2 = 6$. If 400 were expressed as a sum of at least two distinct powers of 2, what would be the least possible sum of the exponents of these powers?\"\n",
    "# solution = \"6\"\n",
    "\n",
    "suffix_ids = tokenizer.encode(solution)\n",
    "prompt_ids = tokenizer.encode(prompt)\n",
    "# keyphrases=[['beach', \"soccer\"]], \n",
    "\n",
    "max_new_tokens = 32\n",
    "min_new_tokens = 5\n",
    "lproc = get_dfa_model_v2(\n",
    "    hmm_model=hmm_model,\n",
    "    prompt_ids=prompt_ids, \n",
    "    tokenizer=tokenizer, \n",
    "    suffix_ids=suffix_ids,\n",
    "    max_new_tokens=max_new_tokens, \n",
    "    min_new_tokens=min_new_tokens,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a5d0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "\n",
    "\n",
    "# set beam_size for beam search; usually the larger the beam_size the\n",
    "# higher the generation quality\n",
    "# beam_size = 8\n",
    "# lproc.hmm_batch_size = beam_size\n",
    "\n",
    "output_gen = model.generate(\n",
    "    input_ids=torch.tensor(prompt_ids).reshape(1, -1),\n",
    "    max_new_tokens=max_new_tokens, \n",
    "    logits_processor=LogitsProcessorList([lproc]),\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    # num_beams=beam_size,\n",
    "    do_sample=True,\n",
    "    length_penalty=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "523ad934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is 10+2? \\xa010+2 = 13 \\xa0* 13 \\xa0 = 13 \\xa0* 13 \\xa0 = 13 \\xa0 13  13  13  13'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_gen[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ebb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
