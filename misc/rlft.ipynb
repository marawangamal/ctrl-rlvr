{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91e827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://verl.readthedocs.io/en/latest/examples/gsm8k_example.html\n",
    "# model_name = 'Qwen/Qwen2.5-0.5B'\n",
    "# model_name = 'google/gemma-2b' # does not seem to work with the chat template\n",
    "model_name = 'allenai/tulu-2-7b'\n",
    "model, tok = AutoModelForCausalLM.from_pretrained(model_name, dtype='bfloat16'), AutoTokenizer.from_pretrained(model_name)\n",
    "tok.chat_template = \"\"\"{% for message in messages %}\n",
    "    {% if message['role'] == 'user' %}\n",
    "    User: {{ message['content'] }}\n",
    "    {% elif message['role'] == 'assistant' %}\n",
    "    Assistant: {{ message['content'] }}\n",
    "    {% endif %}\n",
    "    {% endfor %}Assistant:\"\"\"\n",
    "messages = [{'role': 'user', 'content': 'What is the largest single-digit prime number?'}]\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok)\n",
    "pipe(messages)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
