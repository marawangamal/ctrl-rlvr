{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45a82f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in ./.venv/lib/python3.13/site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from peft) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.13/site-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.13/site-packages (from peft) (2.9.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (from peft) (4.57.1)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.13/site-packages (from peft) (1.11.0)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.13/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in ./.venv/lib/python3.13/site-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers->peft) (0.22.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch transformers datasets tqdm numpy matplotlib\n",
    "!pip install peft\n",
    "# !pip install git+https://github.com/joshuacnf/Ctrl-G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2889f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e6cfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load model and tokenizer\n",
    "# model_name = \"gpt2\"  # Using a small model for demonstration\n",
    "# use gp2 large\n",
    "# model_name = \"gpt2-large\"\n",
    "model_name = \"google/gemma-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    " \n",
    "# Create a reference model (pre-RL state)\n",
    "ref_model = copy.deepcopy(model)\n",
    "\n",
    "# apply lora\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.05,\n",
    "#     target_modules=[\n",
    "#         \"q_proj\",\n",
    "#         \"gate_proj\",\n",
    "#         \"v_proj\",\n",
    "#         \"o_proj\",\n",
    "#         \"k_proj\",\n",
    "#         \"up_proj\",\n",
    "#         \"down_proj\"\n",
    "#     ],\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "# gemma config\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "ref_model.to(device)\n",
    "\n",
    "# print trainable parameters\n",
    "\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842327f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ctrlg.utils.ConstraintLogitsProcessor at 0x139aca4e0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load HMM model and constraints\n",
    "import ctrlg\n",
    "# Load HMM model and constraints\n",
    "import ctrlg\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "# HMM_MODEL_PATH = f'ctrlg/hmm_gpt2-large_common-gen_4096' # alternatively 'ctrlg/hmm_gpt2-large_common-gen_32768' for better quality\n",
    "HMM_MODEL_PATH = snapshot_download(\n",
    "    repo_id=\"gwenweng/gemma\",\n",
    "    local_dir=\"models/gemma\"\n",
    ")\n",
    "hmm_model = ctrlg.HMM.from_pretrained(HMM_MODEL_PATH).to(device)\n",
    "vocab_size = hmm_model.vocab_size\n",
    "eos_token_id = hmm_model.eos_token_id\n",
    "\n",
    "def get_dfa_model(prompt_ids, keyphrases=[[' ']], suffix_ids=None, min_new_tokens=5, max_new_tokens=32):\n",
    "\n",
    "    ##################################### prefix, suffix, prompt #####################################\n",
    "    prefix = '' # generate text starting with nothing\n",
    "    suffix = '.<|endoftext|>' # generate text ending with '<|endoftext|>'; a suffix must end with the eos token\n",
    "    # prompt = '<|endoftext|>' # prompt the base model with the '<|endoftext|>' token\n",
    "\n",
    "    prefix_ids = tokenizer.encode(prefix)\n",
    "    if suffix_ids is None:\n",
    "        suffix_ids = tokenizer.encode(suffix)\n",
    "    # prompt_ids = tokenizer.encode(prompt)\n",
    "    ##################################### prefix, suffix, prompt #####################################\n",
    "\n",
    "\n",
    "    ##################################### DFA Construction #####################################\n",
    "    # ac_builder constructs a DFA representing the constraint that (at least) \n",
    "    # one the patterns must appear; a pattern is a sequence of token ids\n",
    "    ac_builder = ctrlg.AhoCorasickBuilder(vocab_size)\n",
    "\n",
    "    dfa_graphs = []\n",
    "\n",
    "    # constraint 1:\n",
    "    # one of ' riding a bike', ' ride bikes', ' rides a bike', ' biking', ' bikes' has to appear\n",
    "    # AND one of ' park', ' beach' has to appear\n",
    "    # keyphrases = [[' riding a bike', ' ride bikes', ' rides a bike', ' biking', ' bikes'],\n",
    "    #             [' park', ' beach']]\n",
    "    for keyphrase in keyphrases:\n",
    "        patterns = [tokenizer.encode(x) for x in keyphrase]\n",
    "        dfa_graphs.append(ac_builder.build(patterns))\n",
    "\n",
    "    # taking the intersection of the DFAs, i.e., \"logical and\" of the constraints.\n",
    "    # This function also minimizes the constructed DFA, which is mainly CPU-based operations;\n",
    "    # Due to its pure python implemenation, DFA minimization can be slow for complex constraints\n",
    "    dfa_graph = ctrlg.DFA_prod(dfa_graphs, mode='intersection')\n",
    "\n",
    "    # compile the dfa_graph for efficient GPU execution\n",
    "    dfa_model = ctrlg.DFAModel(dfa_graph, vocab_size).to(device)\n",
    "    ##################################### DFA Construction #####################################\n",
    "\n",
    "\n",
    "    ##################################### token length #####################################\n",
    "    # specify the min_new_tokens and max_new_tokens to be generated (excluding\n",
    "    # the prefix and suffix) make sure that the numbers here would not conflict\n",
    "    # with the given constraint: e.g. ask the model to generate 10 words with\n",
    "    # max_new_tokens = 8\n",
    "    # min_new_tokens = 5\n",
    "    # max_new_tokens = 32\n",
    "    ##################################### token length #####################################\n",
    "\n",
    "    # # DEBUG:\n",
    "    # print(f\"prompt_ids shape: {prompt_ids.shape}\")  \n",
    "    # print(f\"prefix_ids shape: {prefix_ids.shape}\")\n",
    "    # print(f\"suffix_ids shape: {suffix_ids.shape}\")\n",
    "\n",
    "    constraint_logits_processor = ctrlg.ConstraintLogitsProcessor(\n",
    "        hmm_model, \n",
    "        dfa_model,\n",
    "        min_new_tokens, \n",
    "        max_new_tokens,\n",
    "        prompt_ids, \n",
    "        prefix_ids=prefix_ids, \n",
    "        suffix_ids=suffix_ids\n",
    "    )\n",
    "\n",
    "    return constraint_logits_processor\n",
    "\n",
    "\n",
    "# test\n",
    "get_dfa_model(prompt_ids=torch.randint(0, tokenizer.vocab_size, (1, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b088ce52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'solution': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def load_gsm8k(n=100):\n",
    "    data = gsm8k[\"train\"].select(range(n))\n",
    "\n",
    "    formatted = []\n",
    "    for item in data:\n",
    "        problem = item[\"question\"]\n",
    "        solution = item[\"answer\"]  # GSM8K solutions are full step-by-step\n",
    "        formatted.append({\n",
    "            \"problem\": problem,\n",
    "            \"solution\": solution,\n",
    "            \"answer\": extract_answer_gsm8k(solution)\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "def generate_math_problems(num_problems=100):\n",
    "    problems = []\n",
    "    for _ in range(num_problems):\n",
    "        a = random.randint(1, 10)\n",
    "        b = random.randint(1, 10)\n",
    "        op = random.choice(['+', '-', '*'])\n",
    "         \n",
    "        if op == '+':\n",
    "            answer = a + b\n",
    "        elif op == '-':\n",
    "            answer = a - b\n",
    "        else:\n",
    "            answer = a * b\n",
    "             \n",
    "        problem = f\"What is {a} {op} {b}?\"\n",
    "        solution = f\"The answer is {answer}.\"\n",
    "        problems.append({\"problem\": problem, \"solution\": solution, \"answer\": answer})\n",
    "     \n",
    "    return problems\n",
    " \n",
    "# math_problems = generate_math_problems(100)\n",
    "math_problems = load_gsm8k(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09761037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_answer(text):\n",
    "    \"\"\"Extract the numerical answer from the text.\"\"\"\n",
    "    try:\n",
    "        # Basic extraction - look for numbers after \"answer is\"\n",
    "        if \"answer is\" in text:\n",
    "            answer_part = text.split(\"answer is\")[1].strip()\n",
    "            # Extract the first number\n",
    "            for word in answer_part.split():\n",
    "                word = word.strip('.,')\n",
    "                if word.isdigit() or (word[0] == '-' and word[1:].isdigit()):\n",
    "                    return int(word)\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_answer_gsm8k(text):\n",
    "    \"\"\"Extract the numerical answer from the text.\"\"\"\n",
    "    try:\n",
    "        # Basic extraction - parse after ####\n",
    "        if \"####\" in text:\n",
    "            answer_part = text.split(\"####\")[1].strip()\n",
    "            # Extract the first number\n",
    "            for word in answer_part.split():\n",
    "                word = word.strip('.,')\n",
    "                if word.isdigit() or (word[0] == '-' and word[1:].isdigit()):\n",
    "                    return int(word)\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    " \n",
    "def compute_reward(response, correct_answer):\n",
    "    \"\"\"Compute reward based on correctness.\"\"\"\n",
    "    extracted = extract_answer(response)\n",
    "    if extracted is not None and extracted == correct_answer:\n",
    "        return 1.0  # Correct answer\n",
    "    return 0.0  # Incorrect answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc1e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "\n",
    "def get_logprobs(model, input_ids, attention_mask):\n",
    "    \"\"\"Get log probabilities for each token.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits[:, :-1, :]  # Remove last position\n",
    "         \n",
    "        # Get log probabilities\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "         \n",
    "        # Get the log probability of the actual next token\n",
    "        target_ids = input_ids[:, 1:]  # Shift right\n",
    "        gathered_logprobs = log_probs.gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "         \n",
    "        # Mask out padding\n",
    "        masked_logprobs = gathered_logprobs * attention_mask[:, 1:].float()\n",
    "         \n",
    "        return masked_logprobs\n",
    " \n",
    "def compute_advantages(rewards, group_rewards):\n",
    "    \"\"\"Compute advantages using the group baseline.\"\"\"\n",
    "    mean_reward = np.mean(group_rewards)\n",
    "    std_reward = np.std(group_rewards) + 1e-8  # Add small epsilon to avoid division by zero\n",
    "     \n",
    "    # Normalize rewards\n",
    "    advantages = (rewards - mean_reward) / std_reward\n",
    "    return advantages\n",
    " \n",
    "def grpo_loss(current_logprobs, old_logprobs, ref_logprobs, advantages, clip_epsilon=0.2, kl_coef=0.1):\n",
    "    \"\"\"Compute the GRPO loss.\"\"\"\n",
    "    # Compute probability ratio\n",
    "    ratio = torch.exp(current_logprobs - old_logprobs)\n",
    "     \n",
    "    # Compute clipped objective\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon)\n",
    "    ppo_obj = torch.min(ratio * advantages, clipped_ratio * advantages)\n",
    "     \n",
    "    # Compute KL divergence term\n",
    "    kl_div = old_logprobs - ref_logprobs\n",
    "     \n",
    "    # Compute final loss\n",
    "    loss = -ppo_obj.mean() + kl_coef * kl_div.mean()\n",
    "     \n",
    "    return loss\n",
    " \n",
    "def train_grpo(model, ref_model, problem, correct_answer, solution, group_size=4, max_length=50):\n",
    "    \"\"\"Train the model using GRPO on a single problem.\"\"\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "     \n",
    "    # Generate responses for the group\n",
    "    group_responses = []\n",
    "    group_rewards = []\n",
    "     \n",
    "    # Create a batch of identical prompts\n",
    "    prompt = f\"Please solve the following math problem: {problem}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # HPs\n",
    "    beam_size = 32\n",
    "    min_new_tokens = 6\n",
    "    max_new_tokens = 32\n",
    "     \n",
    "    # Generate group responses\n",
    "    for _ in range(group_size):\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=max_length + inputs.input_ids.shape[1],\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "            # suffix_ids = tokenizer.encode(solution)\n",
    "            # constraint_logits_processor = get_dfa_model(prompt_ids=inputs.input_ids[0].tolist(), suffix_ids=suffix_ids)\n",
    "            # output_ids = model.generate(\n",
    "            #     input_ids=inputs.input_ids, do_sample=False, length_penalty=0.2,\n",
    "            #     num_beams=beam_size, num_return_sequences=beam_size,\n",
    "            #     min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "            #     logits_processor=LogitsProcessorList([constraint_logits_processor]),\n",
    "            #     pad_token_id=tokenizer.eos_token_id,\n",
    "            # )\n",
    "             \n",
    "            # Get the generated response\n",
    "            response = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            reward = compute_reward(response, correct_answer)\n",
    "             \n",
    "            group_responses.append(response)\n",
    "            group_rewards.append(reward)\n",
    "     \n",
    "    # Compute advantages\n",
    "    advantages = compute_advantages(group_rewards, group_rewards)\n",
    "     \n",
    "    # Optimize the model for each response\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "     \n",
    "    for i in range(group_size):\n",
    "        # Get the full sequence\n",
    "        full_sequence = tokenizer(prompt + group_responses[i], return_tensors=\"pt\").to(device)\n",
    "         \n",
    "        # Get old logprobs\n",
    "        old_logprobs = get_logprobs(model, full_sequence.input_ids, full_sequence.attention_mask)\n",
    "         \n",
    "        # Get reference logprobs\n",
    "        ref_logprobs = get_logprobs(ref_model, full_sequence.input_ids, full_sequence.attention_mask)\n",
    "         \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=full_sequence.input_ids, attention_mask=full_sequence.attention_mask)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "         \n",
    "        # Compute new logprobs\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        target_ids = full_sequence.input_ids[:, 1:]\n",
    "        current_logprobs = log_probs.gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "         \n",
    "        # Mask out padding\n",
    "        mask = full_sequence.attention_mask[:, 1:].float()\n",
    "        current_logprobs = current_logprobs * mask\n",
    "         \n",
    "        # Compute loss\n",
    "        advantage = torch.tensor([advantages[i]]).to(device).expand_as(current_logprobs)\n",
    "        loss = grpo_loss(current_logprobs, old_logprobs, ref_logprobs, advantage)\n",
    "         \n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "         \n",
    "    return np.mean(group_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc97b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/80 [00:00<?, ?it/s]W1116 21:39:21.604000 84096 torch/_dynamo/convert_frame.py:1358] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1116 21:39:21.604000 84096 torch/_dynamo/convert_frame.py:1358] [0/8]    function: 'matmul_a_logb' (/Users/marawangamal/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/ctrlg/utils.py:40)\n",
      "W1116 21:39:21.604000 84096 torch/_dynamo/convert_frame.py:1358] [0/8]    last reason: 0/3: tensor 'A' size mismatch at index 0. expected 4096, actual 3. Guard failed on a parameter, consider using torch._dynamo.config.force_parameter_static_shapes = False to allow dynamism on parameters.\n",
      "W1116 21:39:21.604000 84096 torch/_dynamo/convert_frame.py:1358] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1116 21:39:21.604000 84096 torch/_dynamo/convert_frame.py:1358] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n",
      "Epoch 1/3:   1%|â–         | 1/80 [05:22<7:05:15, 322.98s/it]"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, math_problems, num_samples=10):\n",
    "    \"\"\"Evaluate the model on a subset of math problems.\"\"\"\n",
    "    correct = 0\n",
    "    samples = random.sample(math_problems, min(num_samples, len(math_problems)))\n",
    "     \n",
    "    for problem in samples:\n",
    "        prompt = f\"Please solve the following math problem: {problem['problem']}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=50 + inputs.input_ids.shape[1],\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "             \n",
    "            response = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            reward = compute_reward(response, problem['answer'])\n",
    "            correct += reward\n",
    "     \n",
    "    return correct / len(samples)\n",
    " \n",
    "# Training loop\n",
    "epochs = 3\n",
    "training_problems = math_problems[:80]  # Use 80% for training\n",
    "eval_problems = math_problems[80:]      # Use 20% for evaluation\n",
    " \n",
    "performance_history = []\n",
    " \n",
    "for epoch in range(epochs):\n",
    "    epoch_rewards = []\n",
    "     \n",
    "    for problem in tqdm(training_problems, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        reward = train_grpo(model, ref_model, problem['problem'], problem['answer'], problem['solution'])\n",
    "        epoch_rewards.append(reward)\n",
    "     \n",
    "    # Evaluate model\n",
    "    accuracy = evaluate_model(model, eval_problems)\n",
    "    performance_history.append(accuracy)\n",
    "     \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Average Reward: {np.mean(epoch_rewards):.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7334428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: What is 8 - 1?\n",
      "Model's response: ' \\xa0Answer: 8\\nThe answer is 7.\\nThe answer is 7.\\nThe answer is 7.\\nThe answer is 7.\\nThe answer'\n",
      "Correct answer: 7\n",
      "Reward: 1.0\n",
      "--------------------------------------------------\n",
      "Problem: What is 10 + 5?\n",
      "Model's response: '\\n\\n10 + 5 = 20\\n\\n20 + 5 = 30\\n\\n30 + 5 = 40\\n\\n40 + 5 = 50\\n\\n50 '\n",
      "Correct answer: 15\n",
      "Reward: 0.0\n",
      "--------------------------------------------------\n",
      "Problem: What is 1 - 7?\n",
      "Model's response: '\\n\\nAnswer: ________\\n\\nThe answer is -6.\\n\\nThe answer is -6.\\n\\nThe answer is -6.\\n\\n'\n",
      "Correct answer: -6\n",
      "Reward: 1.0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m(model, num_problems)\u001b[39m\n\u001b[32m     23\u001b[39m suffix_ids = tokenizer.encode(problem[\u001b[33m'\u001b[39m\u001b[33msolution\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     24\u001b[39m constraint_logits_processor = get_dfa_model(prompt_ids=inputs.input_ids[\u001b[32m0\u001b[39m].tolist(), suffix_ids=suffix_ids)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m output_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLogitsProcessorList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconstraint_logits_processor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m response = tokenizer.decode(output_ids[\u001b[32m0\u001b[39m][inputs.input_ids.shape[\u001b[32m1\u001b[39m]:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     34\u001b[39m reward = compute_reward(response, problem[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3265\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3262\u001b[39m flat_running_sequences = \u001b[38;5;28mself\u001b[39m._flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[32m   3263\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m3265\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3267\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3268\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3269\u001b[39m     model_outputs,\n\u001b[32m   3270\u001b[39m     model_kwargs,\n\u001b[32m   3271\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3272\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:1068\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1048\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001b[39;00m\n\u001b[32m   1050\u001b[39m \u001b[33;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1064\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1066\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:925\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    923\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:449\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m residual = hidden_states\n\u001b[32m    448\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_2(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m feed_forward_hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[32m    451\u001b[39m hidden_states = residual + feed_forward_hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:376\u001b[39m, in \u001b[36mGPT2MLP.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    374\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.c_fc(hidden_states)\n\u001b[32m    375\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.act(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/pytorch_utils.py:122\u001b[39m, in \u001b[36mConv1D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    121\u001b[39m     size_out = x.size()[:-\u001b[32m1\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.nf,)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     x = x.view(size_out)\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "\n",
    "max_new_tokens = 32\n",
    "min_new_tokens = 6\n",
    "beam_size = 32\n",
    "\n",
    "def test_model(model, num_problems=5):\n",
    "    \"\"\"Test the model on new math problems.\"\"\"\n",
    "    test_problems = generate_math_problems(num_problems)\n",
    "\n",
    "     \n",
    "    for problem in test_problems:\n",
    "        prompt = f\"Please solve the following math problem: {problem['problem']}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "            # output_ids = model.generate(\n",
    "            #     inputs.input_ids,\n",
    "            #     max_length=50 + inputs.input_ids.shape[1],\n",
    "            #     pad_token_id=tokenizer.eos_token_id,\n",
    "            # )\n",
    "\n",
    "            suffix_ids = tokenizer.encode(problem['solution'])\n",
    "            constraint_logits_processor = get_dfa_model(prompt_ids=inputs.input_ids[0].tolist(), suffix_ids=suffix_ids)\n",
    "            output_ids = model.generate(\n",
    "                input_ids=inputs.input_ids, do_sample=False, length_penalty=0.2,\n",
    "                num_beams=beam_size, num_return_sequences=beam_size,\n",
    "                min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "                logits_processor=LogitsProcessorList([constraint_logits_processor]),\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "             \n",
    "            response = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            reward = compute_reward(response, problem['answer'])\n",
    "             \n",
    "            print(f\"Problem: {problem['problem']}\")\n",
    "            print(f\"Model's response: {repr(response)}\")\n",
    "            print(f\"Correct answer: {problem['answer']}\")\n",
    "            print(f\"Reward: {reward}\")\n",
    "            print(\"-\" * 50)\n",
    " \n",
    "# Test the model\n",
    "test_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "530871ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 3280, 318, 860, 13]\n"
     ]
    }
   ],
   "source": [
    "test_problems = generate_math_problems(5)\n",
    "for problem in test_problems:\n",
    "    prompt = f\"Please solve the following math problem: {problem['problem']}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    break\n",
    "suffix_ids = tokenizer.encode(problem['solution'])\n",
    "print(suffix_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c06383",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(\n",
    "        input_ids=inputs.input_ids, do_sample=False, length_penalty=0.2,\n",
    "    num_beams=beam_size, num_return_sequences=beam_size,\n",
    "    min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "    logits_processor=LogitsProcessorList([constraint_logits_processor]),\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a468784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5492, 8494, 262, 1708, 10688, 1917, 25, 1867, 318, 352, 532, 352, 30]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4b854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "constraint_logits_processor = get_dfa_model(prompt_ids=inputs.input_ids[0].tolist(), keyphrases=[[problem['solution']]])\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs.input_ids, do_sample=False, length_penalty=0.2,\n",
    "    num_beams=beam_size, num_return_sequences=beam_size,\n",
    "    min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "    logits_processor=LogitsProcessorList([constraint_logits_processor]),\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28284ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please solve the following math problem: What is 2 * 5?\\n\\nThe answer is 2.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41e2eb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'What is 2 * 5?', 'solution': 'The answer is 10.', 'answer': 10}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6456e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
