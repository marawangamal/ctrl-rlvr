{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45a82f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in ./.venv/lib/python3.13/site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from peft) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.13/site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.13/site-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.13/site-packages (from peft) (2.9.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (from peft) (4.57.1)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.13/site-packages (from peft) (1.11.0)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.13/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in ./.venv/lib/python3.13/site-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.13/site-packages (from transformers->peft) (0.22.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch transformers datasets tqdm numpy matplotlib\n",
    "!pip install peft\n",
    "# !pip install git+https://github.com/joshuacnf/Ctrl-G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2889f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load model and tokenizer\n",
    "# model_name = \"gpt2\"  # Using a small model for demonstration\n",
    "# use gp2 large\n",
    "# model_name = \"gpt2-large\"\n",
    "model_name = \"google/gemma-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    " \n",
    "# Create a reference model (pre-RL state)\n",
    "ref_model = copy.deepcopy(model)\n",
    "\n",
    "# apply lora\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.05,\n",
    "#     target_modules=[\n",
    "#         \"q_proj\",\n",
    "#         \"gate_proj\",\n",
    "#         \"v_proj\",\n",
    "#         \"o_proj\",\n",
    "#         \"k_proj\",\n",
    "#         \"up_proj\",\n",
    "#         \"down_proj\"\n",
    "#     ],\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "# gemma config\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "ref_model.to(device)\n",
    "\n",
    "# print trainable parameters\n",
    "\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "842327f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "InductorError",
     "evalue": "CppCompileError: C++ compile error\n\nCommand:\nclang++ /var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/hg/chggtlcta6pvwexwktea6pkszd3w5u6myszoiy2l7sulr5dpbk3h.main.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -shared -fPIC -undefined dynamic_lookup -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -include /var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/precompiled_headers/c62bfr35myvt7jik6nuf236zkm2n42zpgvjfrvmt26eeh234b7cl.h -I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13 -I/Users/marawangamal/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/include -I/Users/marawangamal/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -I/opt/homebrew/opt/libomp/include -o /var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/hg/chggtlcta6pvwexwktea6pkszd3w5u6myszoiy2l7sulr5dpbk3h.main.so -lomp -lc10 -L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib -L/Users/marawangamal/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/lib -L/opt/homebrew/opt/libomp/lib\n\nOutput:\nfatal error: file '/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/precompiled_headers/c62bfr35myvt7jik6nuf236zkm2n42zpgvjfrvmt26eeh234b7cl.h' has been modified since the precompiled header '/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/precompiled_headers/c62bfr35myvt7jik6nuf236zkm2n42zpgvjfrvmt26eeh234b7cl.h.pch' was built: mtime changed (was 1763344469, now 1763855015)\nnote: please rebuild precompiled header '/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/precompiled_headers/c62bfr35myvt7jik6nuf236zkm2n42zpgvjfrvmt26eeh234b7cl.h.pch'\n1 error generated.\n\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInductorError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m constraint_logits_processor\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[43mget_dfa_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mget_dfa_model\u001b[39m\u001b[34m(prompt_ids, keyphrases, suffix_ids, min_new_tokens, max_new_tokens)\u001b[39m\n\u001b[32m     55\u001b[39m dfa_model = ctrlg.DFAModel(dfa_graph, vocab_size).to(device)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m##################################### DFA Construction #####################################\u001b[39;00m\n\u001b[32m     57\u001b[39m \n\u001b[32m     58\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# print(f\"prefix_ids shape: {prefix_ids.shape}\")\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# print(f\"suffix_ids shape: {suffix_ids.shape}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m constraint_logits_processor = \u001b[43mctrlg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConstraintLogitsProcessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhmm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdfa_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuffix_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix_ids\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m constraint_logits_processor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/ctrlg/utils.py:140\u001b[39m, in \u001b[36mConstraintLogitsProcessor.__init__\u001b[39m\u001b[34m(self, hmm_model, dfa_model, min_new_tokens, max_new_tokens, prompt_ids, prefix_ids, suffix_ids, temperature, token_ranges, hmm_batch_size)\u001b[39m\n\u001b[32m    137\u001b[39m EV_mask = dfa_model.EV_mask\n\u001b[32m    138\u001b[39m E2Src, E2Dst = dfa_model.E2Src, dfa_model.E2Dst\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m T_weights = \u001b[43mmatmul_a_logb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# num_transitions * hidden_states\u001b[39;00m\n\u001b[32m    141\u001b[39m T_weights.nan_to_num_(neginf=neginf)\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# initialize cache C\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:845\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__cause__\u001b[39;00m  \u001b[38;5;66;03m# User compiler error\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    843\u001b[39m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[32m    844\u001b[39m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m845\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    847\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    848\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:990\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe()).with_traceback(\n\u001b[32m    991\u001b[39m         e.__traceback__\n\u001b[32m    992\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    994\u001b[39m     TritonBundler.end_compile()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:974\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    972\u001b[39m TritonBundler.begin_compile()\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     mb_compiled_graph = \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    978\u001b[39m     mb_compiled_graph._time_taken_ns = time.time_ns() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:1695\u001b[39m, in \u001b[36mfx_codegen_and_compile\u001b[39m\u001b[34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[39m\n\u001b[32m   1691\u001b[39m     fast_scheme = _InProcessFxCompile()\n\u001b[32m   1693\u001b[39m     scheme = _ProgressiveFxCompile(fast_scheme, scheme, progression_configs)\n\u001b[32m-> \u001b[39m\u001b[32m1695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:1505\u001b[39m, in \u001b[36m_InProcessFxCompile.codegen_and_compile\u001b[39m\u001b[34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[39m\n\u001b[32m   1487\u001b[39m         compiled_fn = AotCodeCompiler.compile(\n\u001b[32m   1488\u001b[39m             graph,\n\u001b[32m   1489\u001b[39m             wrapper_code.value,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1502\u001b[39m             ],\n\u001b[32m   1503\u001b[39m         )\n\u001b[32m   1504\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1505\u001b[39m     compiled_module = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m     compiled_fn = compiled_module.call\n\u001b[32m   1507\u001b[39m     compiled_fn_runner = \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   1508\u001b[39m         compiled_module, \u001b[33m\"\u001b[39m\u001b[33mrunner\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1509\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2319\u001b[39m, in \u001b[36mGraphLowering.compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2313\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m   2314\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraphLowering.compile_to_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2315\u001b[39m         phase_name=\u001b[33m\"\u001b[39m\u001b[33mcode_gen\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2316\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2317\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2318\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m2319\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2329\u001b[39m, in \u001b[36mGraphLowering._compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2324\u001b[39m wrapper_code, _ = (\n\u001b[32m   2325\u001b[39m     \u001b[38;5;28mself\u001b[39m.codegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.codegen()\n\u001b[32m   2326\u001b[39m )\n\u001b[32m   2328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, ValueWithLineMap):\n\u001b[32m-> \u001b[39m\u001b[32m2329\u001b[39m     mod = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, FileBackedGraphModule):\n\u001b[32m   2331\u001b[39m     mod = wrapper_code\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2397\u001b[39m, in \u001b[36mGraphLowering._compile_to_module_lines\u001b[39m\u001b[34m(self, wrapper_code)\u001b[39m\n\u001b[32m   2391\u001b[39m     trace_structured(\n\u001b[32m   2392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minductor_output_code\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2393\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m: path},\n\u001b[32m   2394\u001b[39m         payload_fn=\u001b[38;5;28;01mlambda\u001b[39;00m: wrapper_code.value,\n\u001b[32m   2395\u001b[39m     )\n\u001b[32m   2396\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mPyCodeCache.load_by_key_path\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2397\u001b[39m     mod = \u001b[43mPyCodeCache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_by_key_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlinemap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinemap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorchbind_constants\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2402\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2403\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_key = key\n\u001b[32m   2404\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_path = path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:3548\u001b[39m, in \u001b[36mPyCodeCache.load_by_key_path\u001b[39m\u001b[34m(cls, key, path, linemap, attrs)\u001b[39m\n\u001b[32m   3545\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.modules_no_attr[path]\n\u001b[32m   3547\u001b[39m in_toplevel = in_toplevel_process()\n\u001b[32m-> \u001b[39m\u001b[32m3548\u001b[39m mod = \u001b[43m_reload_python_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_sys_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_toplevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3550\u001b[39m \u001b[38;5;66;03m# unzip into separate lines/nodes lists\u001b[39;00m\n\u001b[32m   3551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_toplevel:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/runtime/compile_tasks.py:33\u001b[39m, in \u001b[36m_reload_python_module\u001b[39m\u001b[34m(key, path, set_sys_modules)\u001b[39m\n\u001b[32m     31\u001b[39m mod.\u001b[34m__file__\u001b[39m = path\n\u001b[32m     32\u001b[39m mod.key = key  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m set_sys_modules:\n\u001b[32m     35\u001b[39m     sys.modules[mod.\u001b[34m__name__\u001b[39m] = mod\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/cn/ccnb3td4hzue3lpaspcsls2y3et3qwgmjvd7rvh3vh3himewd5sp.py:139\u001b[39m\n\u001b[32m     33\u001b[39m cpp_fused_amax_exp_sub_0 = async_compile.cpp_pybinding([\u001b[33m'\u001b[39m\u001b[33mconst float*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfloat*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfloat*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconst int64_t\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'''\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[33m#include <torch/csrc/inductor/cpp_prefix.h>\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[33mextern \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  void  kernel(const float* in_ptr0,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33m}\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[33m'''\u001b[39m)\n\u001b[32m     98\u001b[39m cpp_fused_add_log_1 = async_compile.cpp_pybinding([\u001b[33m'\u001b[39m\u001b[33mfloat*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconst float*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconst int64_t\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'''\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[33m#include <torch/csrc/inductor/cpp_prefix.h>\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[33mextern \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  void  kernel(float* in_out_ptr0,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33m}\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[33m'''\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43masync_compile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m async_compile\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRunner\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:631\u001b[39m, in \u001b[36mAsyncCompile.wait\u001b[39m\u001b[34m(self, scope)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_compile_threads() > \u001b[32m1\u001b[39m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m    625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masync_compile.wait\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    626\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m         waitcounter_name_override=\u001b[33m\"\u001b[39m\u001b[33mcompile_triton\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    630\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_futures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m _compile_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:651\u001b[39m, in \u001b[36mAsyncCompile._wait_futures\u001b[39m\u001b[34m(self, scope)\u001b[39m\n\u001b[32m    649\u001b[39m     pbar.set_postfix_str(key)\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     kernel = \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m     scope[key] = kernel\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BrokenProcessPool \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:4289\u001b[39m, in \u001b[36mLambdaFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4288\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Callable[..., Any]:\n\u001b[32m-> \u001b[39m\u001b[32m4289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresult_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:3025\u001b[39m, in \u001b[36mCppPythonBindingsCodeCache.load_pybinding_async.<locals>.future\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3023\u001b[39m \u001b[38;5;28;01mnonlocal\u001b[39;00m result\n\u001b[32m   3024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3025\u001b[39m     result = \u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3026\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, ModuleType)\n\u001b[32m   3027\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;28mcls\u001b[39m.entry_function)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:2821\u001b[39m, in \u001b[36mCppCodeCache.load_async.<locals>.load_fn\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2820\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m future \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2821\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2822\u001b[39m     result = worker_fn()\n\u001b[32m   2823\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:974\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    972\u001b[39m TritonBundler.begin_compile()\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     mb_compiled_graph = \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    978\u001b[39m     mb_compiled_graph._time_taken_ns = time.time_ns() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:1695\u001b[39m, in \u001b[36mfx_codegen_and_compile\u001b[39m\u001b[34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[39m\n\u001b[32m   1691\u001b[39m     fast_scheme = _InProcessFxCompile()\n\u001b[32m   1693\u001b[39m     scheme = _ProgressiveFxCompile(fast_scheme, scheme, progression_configs)\n\u001b[32m-> \u001b[39m\u001b[32m1695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:1505\u001b[39m, in \u001b[36m_InProcessFxCompile.codegen_and_compile\u001b[39m\u001b[34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[39m\n\u001b[32m   1487\u001b[39m         compiled_fn = AotCodeCompiler.compile(\n\u001b[32m   1488\u001b[39m             graph,\n\u001b[32m   1489\u001b[39m             wrapper_code.value,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1502\u001b[39m             ],\n\u001b[32m   1503\u001b[39m         )\n\u001b[32m   1504\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1505\u001b[39m     compiled_module = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m     compiled_fn = compiled_module.call\n\u001b[32m   1507\u001b[39m     compiled_fn_runner = \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   1508\u001b[39m         compiled_module, \u001b[33m\"\u001b[39m\u001b[33mrunner\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1509\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2319\u001b[39m, in \u001b[36mGraphLowering.compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2313\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m   2314\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraphLowering.compile_to_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2315\u001b[39m         phase_name=\u001b[33m\"\u001b[39m\u001b[33mcode_gen\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2316\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2317\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2318\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m2319\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2329\u001b[39m, in \u001b[36mGraphLowering._compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2324\u001b[39m wrapper_code, _ = (\n\u001b[32m   2325\u001b[39m     \u001b[38;5;28mself\u001b[39m.codegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.codegen()\n\u001b[32m   2326\u001b[39m )\n\u001b[32m   2328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, ValueWithLineMap):\n\u001b[32m-> \u001b[39m\u001b[32m2329\u001b[39m     mod = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, FileBackedGraphModule):\n\u001b[32m   2331\u001b[39m     mod = wrapper_code\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2397\u001b[39m, in \u001b[36mGraphLowering._compile_to_module_lines\u001b[39m\u001b[34m(self, wrapper_code)\u001b[39m\n\u001b[32m   2391\u001b[39m     trace_structured(\n\u001b[32m   2392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minductor_output_code\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2393\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m: path},\n\u001b[32m   2394\u001b[39m         payload_fn=\u001b[38;5;28;01mlambda\u001b[39;00m: wrapper_code.value,\n\u001b[32m   2395\u001b[39m     )\n\u001b[32m   2396\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mPyCodeCache.load_by_key_path\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2397\u001b[39m     mod = \u001b[43mPyCodeCache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_by_key_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlinemap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinemap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorchbind_constants\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2402\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2403\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_key = key\n\u001b[32m   2404\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_path = path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:3548\u001b[39m, in \u001b[36mPyCodeCache.load_by_key_path\u001b[39m\u001b[34m(cls, key, path, linemap, attrs)\u001b[39m\n\u001b[32m   3545\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.modules_no_attr[path]\n\u001b[32m   3547\u001b[39m in_toplevel = in_toplevel_process()\n\u001b[32m-> \u001b[39m\u001b[32m3548\u001b[39m mod = \u001b[43m_reload_python_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_sys_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_toplevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3550\u001b[39m \u001b[38;5;66;03m# unzip into separate lines/nodes lists\u001b[39;00m\n\u001b[32m   3551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_toplevel:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/runtime/compile_tasks.py:33\u001b[39m, in \u001b[36m_reload_python_module\u001b[39m\u001b[34m(key, path, set_sys_modules)\u001b[39m\n\u001b[32m     31\u001b[39m mod.\u001b[34m__file__\u001b[39m = path\n\u001b[32m     32\u001b[39m mod.key = key  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m set_sys_modules:\n\u001b[32m     35\u001b[39m     sys.modules[mod.\u001b[34m__name__\u001b[39m] = mod\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/no/cnojvmyuwatkl5l6nlfejb2vx4rvmnx3kmsk3pjam5atmldvdscw.py:139\u001b[39m\n\u001b[32m     33\u001b[39m cpp_fused_amax_exp_sub_0 = async_compile.cpp_pybinding([\u001b[33m'\u001b[39m\u001b[33mconst float*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfloat*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfloat*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconst int64_t\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'''\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[33m#include <torch/csrc/inductor/cpp_prefix.h>\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[33mextern \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  void  kernel(const float* in_ptr0,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33m}\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[33m'''\u001b[39m)\n\u001b[32m     98\u001b[39m cpp_fused_add_log_1 = async_compile.cpp_pybinding([\u001b[33m'\u001b[39m\u001b[33mfloat*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconst float*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconst int64_t\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'''\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[33m#include <torch/csrc/inductor/cpp_prefix.h>\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[33mextern \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  void  kernel(float* in_out_ptr0,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33m}\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[33m'''\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43masync_compile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m async_compile\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRunner\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:631\u001b[39m, in \u001b[36mAsyncCompile.wait\u001b[39m\u001b[34m(self, scope)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_compile_threads() > \u001b[32m1\u001b[39m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m    625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masync_compile.wait\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    626\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m         waitcounter_name_override=\u001b[33m\"\u001b[39m\u001b[33mcompile_triton\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    630\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_futures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m _compile_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:651\u001b[39m, in \u001b[36mAsyncCompile._wait_futures\u001b[39m\u001b[34m(self, scope)\u001b[39m\n\u001b[32m    649\u001b[39m     pbar.set_postfix_str(key)\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     kernel = \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m     scope[key] = kernel\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BrokenProcessPool \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:4289\u001b[39m, in \u001b[36mLambdaFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4288\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Callable[..., Any]:\n\u001b[32m-> \u001b[39m\u001b[32m4289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresult_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:3025\u001b[39m, in \u001b[36mCppPythonBindingsCodeCache.load_pybinding_async.<locals>.future\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3023\u001b[39m \u001b[38;5;28;01mnonlocal\u001b[39;00m result\n\u001b[32m   3024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3025\u001b[39m     result = \u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3026\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, ModuleType)\n\u001b[32m   3027\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;28mcls\u001b[39m.entry_function)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:2821\u001b[39m, in \u001b[36mCppCodeCache.load_async.<locals>.load_fn\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2820\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m future \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2821\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2822\u001b[39m     result = worker_fn()\n\u001b[32m   2823\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:974\u001b[39m, in \u001b[36m_compile_fx_inner\u001b[39m\u001b[34m(gm, example_inputs, **graph_kwargs)\u001b[39m\n\u001b[32m    972\u001b[39m TritonBundler.begin_compile()\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     mb_compiled_graph = \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    978\u001b[39m     mb_compiled_graph._time_taken_ns = time.time_ns() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:1695\u001b[39m, in \u001b[36mfx_codegen_and_compile\u001b[39m\u001b[34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[39m\n\u001b[32m   1691\u001b[39m     fast_scheme = _InProcessFxCompile()\n\u001b[32m   1693\u001b[39m     scheme = _ProgressiveFxCompile(fast_scheme, scheme, progression_configs)\n\u001b[32m-> \u001b[39m\u001b[32m1695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:1505\u001b[39m, in \u001b[36m_InProcessFxCompile.codegen_and_compile\u001b[39m\u001b[34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[39m\n\u001b[32m   1487\u001b[39m         compiled_fn = AotCodeCompiler.compile(\n\u001b[32m   1488\u001b[39m             graph,\n\u001b[32m   1489\u001b[39m             wrapper_code.value,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1502\u001b[39m             ],\n\u001b[32m   1503\u001b[39m         )\n\u001b[32m   1504\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1505\u001b[39m     compiled_module = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m     compiled_fn = compiled_module.call\n\u001b[32m   1507\u001b[39m     compiled_fn_runner = \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   1508\u001b[39m         compiled_module, \u001b[33m\"\u001b[39m\u001b[33mrunner\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1509\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2319\u001b[39m, in \u001b[36mGraphLowering.compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> CompiledModule:\n\u001b[32m   2313\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m   2314\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGraphLowering.compile_to_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2315\u001b[39m         phase_name=\u001b[33m\"\u001b[39m\u001b[33mcode_gen\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2316\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   2317\u001b[39m         dynamo_compile_column_us=\u001b[33m\"\u001b[39m\u001b[33minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2318\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m2319\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2329\u001b[39m, in \u001b[36mGraphLowering._compile_to_module\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2324\u001b[39m wrapper_code, _ = (\n\u001b[32m   2325\u001b[39m     \u001b[38;5;28mself\u001b[39m.codegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.codegen()\n\u001b[32m   2326\u001b[39m )\n\u001b[32m   2328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, ValueWithLineMap):\n\u001b[32m-> \u001b[39m\u001b[32m2329\u001b[39m     mod = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compile_to_module_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, FileBackedGraphModule):\n\u001b[32m   2331\u001b[39m     mod = wrapper_code\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/graph.py:2397\u001b[39m, in \u001b[36mGraphLowering._compile_to_module_lines\u001b[39m\u001b[34m(self, wrapper_code)\u001b[39m\n\u001b[32m   2391\u001b[39m     trace_structured(\n\u001b[32m   2392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minductor_output_code\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2393\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m: path},\n\u001b[32m   2394\u001b[39m         payload_fn=\u001b[38;5;28;01mlambda\u001b[39;00m: wrapper_code.value,\n\u001b[32m   2395\u001b[39m     )\n\u001b[32m   2396\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mPyCodeCache.load_by_key_path\u001b[39m\u001b[33m\"\u001b[39m, log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2397\u001b[39m     mod = \u001b[43mPyCodeCache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_by_key_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlinemap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinemap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorchbind_constants\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2402\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2403\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_key = key\n\u001b[32m   2404\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_path = path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:3548\u001b[39m, in \u001b[36mPyCodeCache.load_by_key_path\u001b[39m\u001b[34m(cls, key, path, linemap, attrs)\u001b[39m\n\u001b[32m   3545\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.modules_no_attr[path]\n\u001b[32m   3547\u001b[39m in_toplevel = in_toplevel_process()\n\u001b[32m-> \u001b[39m\u001b[32m3548\u001b[39m mod = \u001b[43m_reload_python_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_sys_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_toplevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3550\u001b[39m \u001b[38;5;66;03m# unzip into separate lines/nodes lists\u001b[39;00m\n\u001b[32m   3551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_toplevel:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/runtime/compile_tasks.py:33\u001b[39m, in \u001b[36m_reload_python_module\u001b[39m\u001b[34m(key, path, set_sys_modules)\u001b[39m\n\u001b[32m     31\u001b[39m mod.\u001b[34m__file__\u001b[39m = path\n\u001b[32m     32\u001b[39m mod.key = key  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m set_sys_modules:\n\u001b[32m     35\u001b[39m     sys.modules[mod.\u001b[34m__name__\u001b[39m] = mod\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/au/cauam32egv3iwois4fwz6jx3zgf67el3c7rnu2kemhu3ajthnbop.py:139\u001b[39m\n\u001b[32m     33\u001b[39m cpp_fused_amax_exp_sub_0 = async_compile.cpp_pybinding([\u001b[33m'\u001b[39m\u001b[33mconst float*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfloat*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfloat*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconst int64_t\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'''\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[33m#include <torch/csrc/inductor/cpp_prefix.h>\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[33mextern \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  void  kernel(const float* in_ptr0,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m \u001b[33m}\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[33m'''\u001b[39m)\n\u001b[32m     98\u001b[39m cpp_fused_add_log_1 = async_compile.cpp_pybinding([\u001b[33m'\u001b[39m\u001b[33mfloat*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconst float*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconst int64_t\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'''\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[33m#include <torch/csrc/inductor/cpp_prefix.h>\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[33mextern \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  void  kernel(float* in_out_ptr0,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33m}\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[33m'''\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43masync_compile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m async_compile\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mRunner\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:631\u001b[39m, in \u001b[36mAsyncCompile.wait\u001b[39m\u001b[34m(self, scope)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_compile_threads() > \u001b[32m1\u001b[39m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[32m    625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masync_compile.wait\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    626\u001b[39m         log_pt2_compile_event=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m         waitcounter_name_override=\u001b[33m\"\u001b[39m\u001b[33mcompile_triton\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    630\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_futures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m _compile_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/async_compile.py:651\u001b[39m, in \u001b[36mAsyncCompile._wait_futures\u001b[39m\u001b[34m(self, scope)\u001b[39m\n\u001b[32m    649\u001b[39m     pbar.set_postfix_str(key)\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     kernel = \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m     scope[key] = kernel\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BrokenProcessPool \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:4289\u001b[39m, in \u001b[36mLambdaFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4288\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Callable[..., Any]:\n\u001b[32m-> \u001b[39m\u001b[32m4289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresult_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:3025\u001b[39m, in \u001b[36mCppPythonBindingsCodeCache.load_pybinding_async.<locals>.future\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3023\u001b[39m \u001b[38;5;28;01mnonlocal\u001b[39;00m result\n\u001b[32m   3024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3025\u001b[39m     result = \u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3026\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, ModuleType)\n\u001b[32m   3027\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;28mcls\u001b[39m.entry_function)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:2821\u001b[39m, in \u001b[36mCppCodeCache.load_async.<locals>.load_fn\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2820\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m future \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2821\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2822\u001b[39m     result = worker_fn()\n\u001b[32m   2823\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/codecache.py:2851\u001b[39m, in \u001b[36m_worker_compile_cpp\u001b[39m\u001b[34m(lock_path, cpp_builders)\u001b[39m\n\u001b[32m   2849\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m builder \u001b[38;5;129;01min\u001b[39;00m cpp_builders:\n\u001b[32m   2850\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(builder.get_target_file_path()):\n\u001b[32m-> \u001b[39m\u001b[32m2851\u001b[39m         \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py:2020\u001b[39m, in \u001b[36mCppBuilder.build\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2017\u001b[39m _create_if_dir_not_exist(_build_tmp_dir)\n\u001b[32m   2019\u001b[39m build_cmd = \u001b[38;5;28mself\u001b[39m.get_command_line()\n\u001b[32m-> \u001b[39m\u001b[32m2020\u001b[39m \u001b[43mrun_compile_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_build_tmp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2021\u001b[39m _remove_dir(_build_tmp_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py:593\u001b[39m, in \u001b[36mrun_compile_cmd\u001b[39m\u001b[34m(cmd_line, cwd)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_compile_cmd\u001b[39m(cmd_line: \u001b[38;5;28mstr\u001b[39m, cwd: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[33m\"\u001b[39m\u001b[33mcompile_file\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m         \u001b[43m_run_compile_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/_inductor/cpp_builder.py:588\u001b[39m, in \u001b[36m_run_compile_cmd\u001b[39m\u001b[34m(cmd_line, cwd)\u001b[39m\n\u001b[32m    578\u001b[39m     instruction = (\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOpenMP support not found. Please try one of the following solutions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(1) Set the `CXX` environment variable to a compiler other than Apple clang++/g++ \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m with `include/omp.h` under it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    586\u001b[39m     )\n\u001b[32m    587\u001b[39m     output += instruction\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc.CppCompileError(cmd, output) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mInductorError\u001b[39m: CppCompileError: C++ compile error\n\nCommand:\nclang++ /var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/hg/chggtlcta6pvwexwktea6pkszd3w5u6myszoiy2l7sulr5dpbk3h.main.cpp -D TORCH_INDUCTOR_CPP_WRAPPER -D STANDALONE_TORCH_HEADER -D C10_USING_CUSTOM_GENERATED_MACROS -D CPU_CAPABILITY_NEON -D AT_BUILD_ARM_VEC256_WITH_SLEEF -O3 -DNDEBUG -fno-trapping-math -funsafe-math-optimizations -ffinite-math-only -fno-signed-zeros -fno-math-errno -fno-finite-math-only -fno-unsafe-math-optimizations -ffp-contract=off -shared -fPIC -undefined dynamic_lookup -Wall -std=c++17 -Wno-unused-variable -Wno-unknown-pragmas -Werror=ignored-optimization-argument -Xclang -fopenmp -include /var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/precompiled_headers/c62bfr35myvt7jik6nuf236zkm2n42zpgvjfrvmt26eeh234b7cl.h -I/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/include/python3.13 -I/Users/marawangamal/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/include -I/Users/marawangamal/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/include/torch/csrc/api/include -I/opt/homebrew/opt/libomp/include -o /var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/hg/chggtlcta6pvwexwktea6pkszd3w5u6myszoiy2l7sulr5dpbk3h.main.so -lomp -lc10 -L/opt/homebrew/opt/python@3.13/Frameworks/Python.framework/Versions/3.13/lib -L/Users/marawangamal/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/lib -L/opt/homebrew/opt/libomp/lib\n\nOutput:\nfatal error: file '/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/precompiled_headers/c62bfr35myvt7jik6nuf236zkm2n42zpgvjfrvmt26eeh234b7cl.h' has been modified since the precompiled header '/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/precompiled_headers/c62bfr35myvt7jik6nuf236zkm2n42zpgvjfrvmt26eeh234b7cl.h.pch' was built: mtime changed (was 1763344469, now 1763855015)\nnote: please rebuild precompiled header '/var/folders/r_/d81gvkws1n5fg2_7mb57cwzh0000gn/T/torchinductor_marawangamal/precompiled_headers/c62bfr35myvt7jik6nuf236zkm2n42zpgvjfrvmt26eeh234b7cl.h.pch'\n1 error generated.\n\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "# Load HMM model and constraints\n",
    "import ctrlg\n",
    "# Load HMM model and constraints\n",
    "import ctrlg\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "HMM_MODEL_PATH = f'ctrlg/hmm_gpt2-large_common-gen_4096' # alternatively 'ctrlg/hmm_gpt2-large_common-gen_32768' for better quality\n",
    "# HMM_MODEL_PATH = snapshot_download(\n",
    "#     repo_id=\"gwenweng/gemma\",\n",
    "#     local_dir=\"models/gemma\"\n",
    "# )\n",
    "hmm_model = ctrlg.HMM.from_pretrained(HMM_MODEL_PATH).to(device)\n",
    "vocab_size = hmm_model.vocab_size\n",
    "eos_token_id = hmm_model.eos_token_id\n",
    "\n",
    "def get_dfa_model(prompt_ids, keyphrases=[[' ']], suffix_ids=None, min_new_tokens=5, max_new_tokens=32):\n",
    "\n",
    "    ##################################### prefix, suffix, prompt #####################################\n",
    "    prefix = '' # generate text starting with nothing\n",
    "    suffix = '.<|endoftext|>' # generate text ending with '<|endoftext|>'; a suffix must end with the eos token\n",
    "    # prompt = '<|endoftext|>' # prompt the base model with the '<|endoftext|>' token\n",
    "\n",
    "    prefix_ids = tokenizer.encode(prefix)\n",
    "    if suffix_ids is None:\n",
    "        suffix_ids = tokenizer.encode(suffix)\n",
    "    # prompt_ids = tokenizer.encode(prompt)\n",
    "    ##################################### prefix, suffix, prompt #####################################\n",
    "\n",
    "\n",
    "    ##################################### DFA Construction #####################################\n",
    "    # ac_builder constructs a DFA representing the constraint that (at least) \n",
    "    # one the patterns must appear; a pattern is a sequence of token ids\n",
    "    ac_builder = ctrlg.AhoCorasickBuilder(vocab_size)\n",
    "\n",
    "    dfa_graphs = []\n",
    "\n",
    "    # constraint 1:\n",
    "    # one of ' riding a bike', ' ride bikes', ' rides a bike', ' biking', ' bikes' has to appear\n",
    "    # AND one of ' park', ' beach' has to appear\n",
    "    # keyphrases = [[' riding a bike', ' ride bikes', ' rides a bike', ' biking', ' bikes'],\n",
    "    #             [' park', ' beach']]\n",
    "    for keyphrase in keyphrases:\n",
    "        patterns = [tokenizer.encode(x) for x in keyphrase]\n",
    "        dfa_graphs.append(ac_builder.build(patterns))\n",
    "\n",
    "    # taking the intersection of the DFAs, i.e., \"logical and\" of the constraints.\n",
    "    # This function also minimizes the constructed DFA, which is mainly CPU-based operations;\n",
    "    # Due to its pure python implemenation, DFA minimization can be slow for complex constraints\n",
    "    dfa_graph = ctrlg.DFA_prod(dfa_graphs, mode='intersection')\n",
    "\n",
    "    # compile the dfa_graph for efficient GPU execution\n",
    "    dfa_model = ctrlg.DFAModel(dfa_graph, vocab_size).to(device)\n",
    "    ##################################### DFA Construction #####################################\n",
    "\n",
    "\n",
    "    ##################################### token length #####################################\n",
    "    # specify the min_new_tokens and max_new_tokens to be generated (excluding\n",
    "    # the prefix and suffix) make sure that the numbers here would not conflict\n",
    "    # with the given constraint: e.g. ask the model to generate 10 words with\n",
    "    # max_new_tokens = 8\n",
    "    # min_new_tokens = 5\n",
    "    # max_new_tokens = 32\n",
    "    ##################################### token length #####################################\n",
    "\n",
    "    # # DEBUG:\n",
    "    # print(f\"prompt_ids shape: {prompt_ids.shape}\")  \n",
    "    # print(f\"prefix_ids shape: {prefix_ids.shape}\")\n",
    "    # print(f\"suffix_ids shape: {suffix_ids.shape}\")\n",
    "\n",
    "    constraint_logits_processor = ctrlg.ConstraintLogitsProcessor(\n",
    "        hmm_model, \n",
    "        dfa_model,\n",
    "        min_new_tokens, \n",
    "        max_new_tokens,\n",
    "        prompt_ids, \n",
    "        prefix_ids=prefix_ids, \n",
    "        suffix_ids=suffix_ids\n",
    "    )\n",
    "\n",
    "    return constraint_logits_processor\n",
    "\n",
    "\n",
    "# test\n",
    "get_dfa_model(prompt_ids=torch.randint(0, tokenizer.vocab_size, (1, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b088ce52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'solution': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def load_gsm8k(n=100):\n",
    "    data = gsm8k[\"train\"].select(range(n))\n",
    "\n",
    "    formatted = []\n",
    "    for item in data:\n",
    "        problem = item[\"question\"]\n",
    "        solution = item[\"answer\"]  # GSM8K solutions are full step-by-step\n",
    "        formatted.append({\n",
    "            \"problem\": problem,\n",
    "            \"solution\": solution,\n",
    "            \"answer\": extract_answer_gsm8k(solution)\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "def generate_math_problems(num_problems=100):\n",
    "    problems = []\n",
    "    for _ in range(num_problems):\n",
    "        a = random.randint(1, 10)\n",
    "        b = random.randint(1, 10)\n",
    "        op = random.choice(['+', '-', '*'])\n",
    "         \n",
    "        if op == '+':\n",
    "            answer = a + b\n",
    "        elif op == '-':\n",
    "            answer = a - b\n",
    "        else:\n",
    "            answer = a * b\n",
    "             \n",
    "        problem = f\"What is {a} {op} {b}?\"\n",
    "        solution = f\"The answer is {answer}.\"\n",
    "        problems.append({\"problem\": problem, \"solution\": solution, \"answer\": answer})\n",
    "     \n",
    "    return problems\n",
    " \n",
    "# math_problems = generate_math_problems(100)\n",
    "math_problems = load_gsm8k(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09761037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_answer(text):\n",
    "    \"\"\"Extract the numerical answer from the text.\"\"\"\n",
    "    try:\n",
    "        # Basic extraction - look for numbers after \"answer is\"\n",
    "        if \"answer is\" in text:\n",
    "            answer_part = text.split(\"answer is\")[1].strip()\n",
    "            # Extract the first number\n",
    "            for word in answer_part.split():\n",
    "                word = word.strip('.,')\n",
    "                if word.isdigit() or (word[0] == '-' and word[1:].isdigit()):\n",
    "                    return int(word)\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_answer_gsm8k(text):\n",
    "    \"\"\"Extract the numerical answer from the text.\"\"\"\n",
    "    try:\n",
    "        # Basic extraction - parse after ####\n",
    "        if \"####\" in text:\n",
    "            answer_part = text.split(\"####\")[1].strip()\n",
    "            # Extract the first number\n",
    "            for word in answer_part.split():\n",
    "                word = word.strip('.,')\n",
    "                if word.isdigit() or (word[0] == '-' and word[1:].isdigit()):\n",
    "                    return int(word)\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    " \n",
    "def compute_reward(response, correct_answer):\n",
    "    \"\"\"Compute reward based on correctness.\"\"\"\n",
    "    extracted = extract_answer(response)\n",
    "    if extracted is not None and extracted == correct_answer:\n",
    "        return 1.0  # Correct answer\n",
    "    return 0.0  # Incorrect answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc1e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "\n",
    "def get_logprobs(model, input_ids, attention_mask):\n",
    "    \"\"\"Get log probabilities for each token.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits[:, :-1, :]  # Remove last position\n",
    "         \n",
    "        # Get log probabilities\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "         \n",
    "        # Get the log probability of the actual next token\n",
    "        target_ids = input_ids[:, 1:]  # Shift right\n",
    "        gathered_logprobs = log_probs.gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "         \n",
    "        # Mask out padding\n",
    "        masked_logprobs = gathered_logprobs * attention_mask[:, 1:].float()\n",
    "         \n",
    "        return masked_logprobs\n",
    " \n",
    "def compute_advantages(rewards, group_rewards):\n",
    "    \"\"\"Compute advantages using the group baseline.\"\"\"\n",
    "    mean_reward = np.mean(group_rewards)\n",
    "    std_reward = np.std(group_rewards) + 1e-8  # Add small epsilon to avoid division by zero\n",
    "     \n",
    "    # Normalize rewards\n",
    "    advantages = (rewards - mean_reward) / std_reward\n",
    "    return advantages\n",
    " \n",
    "def grpo_loss(current_logprobs, old_logprobs, ref_logprobs, advantages, clip_epsilon=0.2, kl_coef=0.1):\n",
    "    \"\"\"Compute the GRPO loss.\"\"\"\n",
    "    # Compute probability ratio\n",
    "    ratio = torch.exp(current_logprobs - old_logprobs)\n",
    "     \n",
    "    # Compute clipped objective\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon)\n",
    "    ppo_obj = torch.min(ratio * advantages, clipped_ratio * advantages)\n",
    "     \n",
    "    # Compute KL divergence term\n",
    "    kl_div = old_logprobs - ref_logprobs\n",
    "     \n",
    "    # Compute final loss\n",
    "    loss = -ppo_obj.mean() + kl_coef * kl_div.mean()\n",
    "     \n",
    "    return loss\n",
    " \n",
    "def train_grpo(model, ref_model, problem, correct_answer, solution, group_size=4, max_length=50):\n",
    "    \"\"\"Train the model using GRPO on a single problem.\"\"\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "     \n",
    "    # Generate responses for the group\n",
    "    group_responses = []\n",
    "    group_rewards = []\n",
    "     \n",
    "    # Create a batch of identical prompts\n",
    "    prompt = f\"Please solve the following math problem: {problem}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    # HPs\n",
    "    beam_size = 32\n",
    "    min_new_tokens = 6\n",
    "    max_new_tokens = 32\n",
    "     \n",
    "    # Generate group responses\n",
    "    for _ in range(group_size):\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_length=max_length + inputs.input_ids.shape[1],\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "            # suffix_ids = tokenizer.encode(solution)\n",
    "            # constraint_logits_processor = get_dfa_model(prompt_ids=inputs.input_ids[0].tolist(), suffix_ids=suffix_ids)\n",
    "            # output_ids = model.generate(\n",
    "            #     input_ids=inputs.input_ids, do_sample=False, length_penalty=0.2,\n",
    "            #     num_beams=beam_size, num_return_sequences=beam_size,\n",
    "            #     min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "            #     logits_processor=LogitsProcessorList([constraint_logits_processor]),\n",
    "            #     pad_token_id=tokenizer.eos_token_id,\n",
    "            # )\n",
    "             \n",
    "            # Get the generated response\n",
    "            response = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            reward = compute_reward(response, correct_answer)\n",
    "             \n",
    "            group_responses.append(response)\n",
    "            group_rewards.append(reward)\n",
    "     \n",
    "    # Compute advantages\n",
    "    advantages = compute_advantages(group_rewards, group_rewards)\n",
    "     \n",
    "    # Optimize the model for each response\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "     \n",
    "    for i in range(group_size):\n",
    "        # Get the full sequence\n",
    "        full_sequence = tokenizer(prompt + group_responses[i], return_tensors=\"pt\").to(device)\n",
    "         \n",
    "        # Get old logprobs\n",
    "        old_logprobs = get_logprobs(model, full_sequence.input_ids, full_sequence.attention_mask)\n",
    "         \n",
    "        # Get reference logprobs\n",
    "        ref_logprobs = get_logprobs(ref_model, full_sequence.input_ids, full_sequence.attention_mask)\n",
    "         \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=full_sequence.input_ids, attention_mask=full_sequence.attention_mask)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "         \n",
    "        # Compute new logprobs\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        target_ids = full_sequence.input_ids[:, 1:]\n",
    "        current_logprobs = log_probs.gather(dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "         \n",
    "        # Mask out padding\n",
    "        mask = full_sequence.attention_mask[:, 1:].float()\n",
    "        current_logprobs = current_logprobs * mask\n",
    "         \n",
    "        # Compute loss\n",
    "        advantage = torch.tensor([advantages[i]]).to(device).expand_as(current_logprobs)\n",
    "        loss = grpo_loss(current_logprobs, old_logprobs, ref_logprobs, advantage)\n",
    "         \n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "         \n",
    "    return np.mean(group_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc97b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/80 [00:00<?, ?it/s]W1116 21:39:21.604000 84096 torch/_dynamo/convert_frame.py:1358] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1116 21:39:21.604000 84096 torch/_dynamo/convert_frame.py:1358] [0/8]    function: 'matmul_a_logb' (/Users/marawangamal/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/ctrlg/utils.py:40)\n",
      "W1116 21:39:21.604000 84096 torch/_dynamo/convert_frame.py:1358] [0/8]    last reason: 0/3: tensor 'A' size mismatch at index 0. expected 4096, actual 3. Guard failed on a parameter, consider using torch._dynamo.config.force_parameter_static_shapes = False to allow dynamism on parameters.\n",
      "W1116 21:39:21.604000 84096 torch/_dynamo/convert_frame.py:1358] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1116 21:39:21.604000 84096 torch/_dynamo/convert_frame.py:1358] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n",
      "Epoch 1/3:   1%|         | 1/80 [05:22<7:05:15, 322.98s/it]"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, math_problems, num_samples=10):\n",
    "    \"\"\"Evaluate the model on a subset of math problems.\"\"\"\n",
    "    correct = 0\n",
    "    samples = random.sample(math_problems, min(num_samples, len(math_problems)))\n",
    "     \n",
    "    for problem in samples:\n",
    "        prompt = f\"Please solve the following math problem: {problem['problem']}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=50 + inputs.input_ids.shape[1],\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "             \n",
    "            response = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            reward = compute_reward(response, problem['answer'])\n",
    "            correct += reward\n",
    "     \n",
    "    return correct / len(samples)\n",
    " \n",
    "# Training loop\n",
    "epochs = 3\n",
    "training_problems = math_problems[:80]  # Use 80% for training\n",
    "eval_problems = math_problems[80:]      # Use 20% for evaluation\n",
    " \n",
    "performance_history = []\n",
    " \n",
    "for epoch in range(epochs):\n",
    "    epoch_rewards = []\n",
    "     \n",
    "    for problem in tqdm(training_problems, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        reward = train_grpo(model, ref_model, problem['problem'], problem['answer'], problem['solution'])\n",
    "        epoch_rewards.append(reward)\n",
    "     \n",
    "    # Evaluate model\n",
    "    accuracy = evaluate_model(model, eval_problems)\n",
    "    performance_history.append(accuracy)\n",
    "     \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Average Reward: {np.mean(epoch_rewards):.4f}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7334428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: What is 8 - 1?\n",
      "Model's response: ' \\xa0Answer: 8\\nThe answer is 7.\\nThe answer is 7.\\nThe answer is 7.\\nThe answer is 7.\\nThe answer'\n",
      "Correct answer: 7\n",
      "Reward: 1.0\n",
      "--------------------------------------------------\n",
      "Problem: What is 10 + 5?\n",
      "Model's response: '\\n\\n10 + 5 = 20\\n\\n20 + 5 = 30\\n\\n30 + 5 = 40\\n\\n40 + 5 = 50\\n\\n50 '\n",
      "Correct answer: 15\n",
      "Reward: 0.0\n",
      "--------------------------------------------------\n",
      "Problem: What is 1 - 7?\n",
      "Model's response: '\\n\\nAnswer: ________\\n\\nThe answer is -6.\\n\\nThe answer is -6.\\n\\nThe answer is -6.\\n\\n'\n",
      "Correct answer: -6\n",
      "Reward: 1.0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m(model, num_problems)\u001b[39m\n\u001b[32m     23\u001b[39m suffix_ids = tokenizer.encode(problem[\u001b[33m'\u001b[39m\u001b[33msolution\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     24\u001b[39m constraint_logits_processor = get_dfa_model(prompt_ids=inputs.input_ids[\u001b[32m0\u001b[39m].tolist(), suffix_ids=suffix_ids)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m output_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLogitsProcessorList\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconstraint_logits_processor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m response = tokenizer.decode(output_ids[\u001b[32m0\u001b[39m][inputs.input_ids.shape[\u001b[32m1\u001b[39m]:], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     34\u001b[39m reward = compute_reward(response, problem[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:3265\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3262\u001b[39m flat_running_sequences = \u001b[38;5;28mself\u001b[39m._flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[32m   3263\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m3265\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3267\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3268\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3269\u001b[39m     model_outputs,\n\u001b[32m   3270\u001b[39m     model_kwargs,\n\u001b[32m   3271\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3272\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:1068\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1048\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001b[39;00m\n\u001b[32m   1050\u001b[39m \u001b[33;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1064\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1066\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:925\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    923\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:449\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m residual = hidden_states\n\u001b[32m    448\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_2(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m feed_forward_hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[32m    451\u001b[39m hidden_states = residual + feed_forward_hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:376\u001b[39m, in \u001b[36mGPT2MLP.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    374\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.c_fc(hidden_states)\n\u001b[32m    375\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.act(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/ctrl-rlvr/.venv/lib/python3.13/site-packages/transformers/pytorch_utils.py:122\u001b[39m, in \u001b[36mConv1D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    121\u001b[39m     size_out = x.size()[:-\u001b[32m1\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.nf,)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     x = x.view(size_out)\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "\n",
    "max_new_tokens = 32\n",
    "min_new_tokens = 6\n",
    "beam_size = 32\n",
    "\n",
    "def test_model(model, num_problems=5):\n",
    "    \"\"\"Test the model on new math problems.\"\"\"\n",
    "    test_problems = generate_math_problems(num_problems)\n",
    "\n",
    "     \n",
    "    for problem in test_problems:\n",
    "        prompt = f\"Please solve the following math problem: {problem['problem']}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "            # output_ids = model.generate(\n",
    "            #     inputs.input_ids,\n",
    "            #     max_length=50 + inputs.input_ids.shape[1],\n",
    "            #     pad_token_id=tokenizer.eos_token_id,\n",
    "            # )\n",
    "\n",
    "            suffix_ids = tokenizer.encode(problem['solution'])\n",
    "            constraint_logits_processor = get_dfa_model(prompt_ids=inputs.input_ids[0].tolist(), suffix_ids=suffix_ids)\n",
    "            output_ids = model.generate(\n",
    "                input_ids=inputs.input_ids, do_sample=False, length_penalty=0.2,\n",
    "                num_beams=beam_size, num_return_sequences=beam_size,\n",
    "                min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "                logits_processor=LogitsProcessorList([constraint_logits_processor]),\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "             \n",
    "            response = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            reward = compute_reward(response, problem['answer'])\n",
    "             \n",
    "            print(f\"Problem: {problem['problem']}\")\n",
    "            print(f\"Model's response: {repr(response)}\")\n",
    "            print(f\"Correct answer: {problem['answer']}\")\n",
    "            print(f\"Reward: {reward}\")\n",
    "            print(\"-\" * 50)\n",
    " \n",
    "# Test the model\n",
    "test_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "530871ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 3280, 318, 860, 13]\n"
     ]
    }
   ],
   "source": [
    "test_problems = generate_math_problems(5)\n",
    "for problem in test_problems:\n",
    "    prompt = f\"Please solve the following math problem: {problem['problem']}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    break\n",
    "suffix_ids = tokenizer.encode(problem['solution'])\n",
    "print(suffix_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c06383",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model.generate(\n",
    "        input_ids=inputs.input_ids, do_sample=False, length_penalty=0.2,\n",
    "    num_beams=beam_size, num_return_sequences=beam_size,\n",
    "    min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "    logits_processor=LogitsProcessorList([constraint_logits_processor]),\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a468784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5492, 8494, 262, 1708, 10688, 1917, 25, 1867, 318, 352, 532, 352, 30]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4b854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "constraint_logits_processor = get_dfa_model(prompt_ids=inputs.input_ids[0].tolist(), keyphrases=[[problem['solution']]])\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs.input_ids, do_sample=False, length_penalty=0.2,\n",
    "    num_beams=beam_size, num_return_sequences=beam_size,\n",
    "    min_new_tokens=min_new_tokens, max_new_tokens=max_new_tokens,\n",
    "    logits_processor=LogitsProcessorList([constraint_logits_processor]),\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28284ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please solve the following math problem: What is 2 * 5?\\n\\nThe answer is 2.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41e2eb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'What is 2 * 5?', 'solution': 'The answer is 10.', 'answer': 10}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6456e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
