# Experiment 1
# Evaluating efficacy of multi-token regularization during pretraining and finetuning
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:rtx8000:1"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=16"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/crl/.venv/bin/activate"

  gtamia:   
    - "#!/bin/bash"
    # Single node w/ 4 GPUs
    - "#SBATCH --gres=gpu:4"
    # Multi-node w/ 4 GPUs per node
    # - "#SBATCH --nodes=2"
    # - "#SBATCH --gpus-per-node=h100:4"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=16"
    - "#SBATCH --time=24:00:00" # 24 hours
    - "module load cuda/12.6 arrow python/3.12 httpproxy"
    - "source /scratch/m/mgamal/crl/.venv/bin/activate"

group:
  name: "ctrl-grpo"
  type: sequential
  jobs:
    # - group:
    #     type: sweep
    #     preamble: gtamia
    #     sweep:
    #       model: ["allenai/tulu-2-7b"]
    #     sweep_template:  "accelerate launch train_grpo.py --model {model}"

    - group: 
        name: "gemma-2b"
        type: parallel
        jobs:
          - job:
              preamble: gtamia
              command: "accelerate launch train_grpo.py --model google/gemma-2b"

          - job:
              preamble: gtamia
              command: "accelerate launch train_grpo.py --model google/gemma-2b"
