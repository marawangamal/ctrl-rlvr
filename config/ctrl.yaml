# Experiment 1
# Evaluating efficacy of multi-token regularization during pretraining and finetuning
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:rtx8000:1"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=16"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/crl/.venv/bin/activate"

  gtamia:   
    - "#!/bin/bash"
    - "#SBATCH --gres=gpu:4"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=16"
    - "module load cuda/12.6 arrow python/3.12 httpproxy"
    - "source /scratch/m/mgamal/crl/.venv/bin/activate"

group:
  name: "ctrl-grpo"
  type: sequential
  jobs:
    - group: 
        name: "training"
        type: parallel
        jobs:
          - job:
              preamble: gtamia
              command: "python train_grpo.py"

          # - job:
          #     preamble: gtamia
          #     command: "python train.py --model vgg --lr 0.001 --epochs 100"
