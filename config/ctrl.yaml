# Experiment 1
# Evaluating efficacy of multi-token regularization during pretraining and finetuning
preambles:
  glong:   
    - "#!/bin/bash"
    - "#SBATCH --output=slurm/slurm-%j.out"
    - "#SBATCH --error=slurm/slurm-%j.err"   
    - "#SBATCH --partition=long"
    - "#SBATCH --gres=gpu:rtx8000:1"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=16"
    - "#SBATCH --nodes=1"
    - "source /home/mila/m/marawan.gamal/scratch/crl/.venv/bin/activate"

  gtamia:   
    - "#!/bin/bash"
    # Single node w/ 4 GPUs
    - "#SBATCH --gres=gpu:4"
    # Multi-node w/ 4 GPUs per node
    # - "#SBATCH --nodes=2"
    # - "#SBATCH --gpus-per-node=h100:4"
    - "#SBATCH --mem=128G"
    - "#SBATCH --cpus-per-task=16"
    - "#SBATCH --time=24:00:00" # 24 hours
    - "module load cuda/12.6 arrow python/3.12 httpproxy"
    - "source /scratch/m/mgamal/crl/.venv/bin/activate"

group:
  name: "ctrl-grpo"
  type: sequential
  jobs:
    - group:
        type: sweep
        preamble: glong
        sweep:
          model: ["gpt2-large"]
          constraint_mode: ["suffix", "keyphrase"]
          hmm: ["ctrlg/hmm_gpt2-large_common-gen_4096", "ctrlg/hmm_gpt2-large_common-gen_32768"]
          dataset_name: ["gsm8k"]
        sweep_template:  "python train_grpo.py --model {model} --constraint_mode {constraint_mode} --batch_size 8 --hmm {hmm}"


# Eg.
# python train_grpo.py --model gpt2-large --constraint_mode suffix --batch_size 8 --hmm ctrlg/hmm_gpt2-large_common-gen_4096 --dataset_name gsm8k
    # - group: 
    #     name: "gemma-2b"
    #     type: parallel
    #     jobs:
    #       - job:
    #           preamble: gtamia
    #           command: "accelerate launch train_grpo.py --model google/gemma-2b"

    #       - job:
    #           preamble: gtamia
    #           command: "accelerate launch train_grpo.py --model google/gemma-2b"
